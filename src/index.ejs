<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
  <style type="text/css">@import "vis/playground.css";</style>
  
  <script src="vis/js/d3.min.js" charset="utf-8"></script>
  <script src="vis/js/underscore.js" charset="utf-8"></script>
  <script src="vis/js/rounding.js" charset="utf-8"></script>
  <script src="vis/util.js" charset="utf-8"></script>
  <script src="vis/GridWorld.js" charset="utf-8"></script>
  <script src="vis/LearnQV.js" charset="utf-8"></script>
  <script src="vis/Policy.js" charset="utf-8"></script>
  <script src="vis/Aprox.js" charset="utf-8"></script>

  <script src="vis/cliff.js" charset="utf-8"></script>
  <script src="vis/compare.js" charset="utf-8"></script>
  <script src="vis/tug.js" charset="utf-8"></script>
  <script src="vis/tug_baseline.js" charset="utf-8"></script>
  <script src="vis/reinforce.js" charset="utf-8"></script>
</head>


<body>

<d-front-matter>
  <script type="text/json">{
  "title": "The Paths Perspective on Value Learning",
  "description": "",
  "password": "td",
  "authors": [

    {
      "author": "Chris Olah",
      "authorURL": "https://colah.github.io/",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    },

    {
      "author": "Sam Greydanus",
      "authorURL": "https://greydanus.github.io/about.html",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    },

    {
      "author": "Justin Gilmer",
      "authorURL": "https://scholar.google.com/citations?user=Ml_vQ8MAAAAJ&hl=en",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    }
  ],
  "katex": {
    "strict": false,
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<style>
  #cliff_iter .cell rect {
    fill: #e7eae7;
  }
  .cell rect {
    fill: #e7eae7;
  }
</style>

<d-title>
  <h1>The Paths Perspective on Value Learning</h1>
  <p>
    A closer look at how Temporal Difference (TD) learning merges paths of experience for greater statistical efficiency.
  </p>
</d-title>

  <figure>
    <div id="compare_hero" style="width:1000px; height:350px; margin:auto; position:relative;"></div>
    <script> compare_vis(d3.select("#compare_hero"), {algs: ["MC", "TD", "Q"] }); </script> 
  </figure>

  <center> <button type="button" onclick='alert("This button does not work yet.")'>Run</button> </center>

<d-article>
<!-- <d-cite key="Tsividis2017HumanAtari"></d-cite> -->
  
  <h2>Introduction</h2>
  <p>
    In the last few years, reinforcement learning (RL) has made remarkable progress, including <a href="https://deepmind.com/research/alphago/">beating world-champion Go players</a>, <a href="https://blog.openai.com/learning-dexterity/">controlling robotic hands</a>, and even <a href="https://deepmind.com/blog/learning-to-generate-images/">painting pictures</a>.
  </p>

  <p>
    One of the key sub-problems of RL is value estimation – learning the long-term consequences of being in a state.

    This can be tricky because future returns are generally noisy, affected by many things other than the present state. The further we look into the future, the more this becomes true.

    But while difficult, estimating value is also essential to most approaches to RL.<d-footnote>For many approaches (policy-value iteration), estimating value essentially is the whole problem, while in other approaches (actor-critic models), value estimation is essential for reducing noise.</d-footnote>
  </p>

  <p>
    The natural way to estimate the value of a state is as the average return you observe from that state. We call this Monte Carlo value estimation. 
  </p>


  <figure id="cliffworld-mc" class="l-body" style="margin-top: 0; margin-bottom: 0; width: 100%;">

      <img style="width: 30%; margin: 1%; " src="figures/cliffworld-path1.svg"/>
      <img style="width: 30%; margin: 1%; " src="figures/cliffworld-path2.svg"/>
      <img style="width: 30%; margin: 1%; " src="figures/cliffworld-mc.svg"/>

      <figcaption>
        <div class="eq-grid" style="grid-gap: 20%; width: 100%; float: right;">
          <figcaption style="grid-row: 1; grid-column: 1;">
            Sometimes the agent reaches its goal.
          </figcaption>
          <figcaption style="grid-row: 1; grid-column: 2;">
            Other times it falls off the cliff.
          </figcaption>
          <figcaption style="grid-row: 1; grid-column: 3;">
            Monte Carlo averages over trajectories where they intersect.
          </figcaption>
        </div>
      </figcaption>
  </figure>

  <figcaption class="l-gutter">
    <b>Cliff World</b> <d-cite key="?"></d-cite> is a classic RL example, where the agent learns to walk along a <span style="background: #FFD5D5">cliff</span> to reach a <span style="background: #D5D5FF">goal</span>.
  </figcaption>

  <p>
    If a state is visited by only one episode, Monte Carlo says its value is the return of that episode. If multiple episodes visit a state, Monte Carlo estimates its value to be the average over them.
  </p>

  <p>
    Let's write Monte Carlo a bit more formally.
    In RL, we often describe algorithms with update rules, which tell us how estimates change with one more episode.
    We'll use an "updates toward" ($\hookleftarrow$) operator to keep equations simple.
  </p>

  <div class="eq-grid" style="grid-gap: 8px;">
      <div style="grid-row: 1; grid-column: 1;"><d-math>V(s_t)~~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>\hookleftarrow~~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>R_t</d-math></div>

      <figcaption style="grid-row: 2; grid-column: 1; max-width:140px;"> State value </figcaption>
      <figcaption style="grid-row: 2; grid-column: 5; max-width:140px;"> Return </figcaption>
  </div>

  <p>
    Estimating value in this way makes a lot of sense. In fact, it might be surprising that we can do better.
  </p>

  <h2>Beating Monte Carlo</h2>

    <p>
      But we <i>can</i> do better! The trick is to use a method called <i>Temporal Difference (TD) learning</i> which bootstraps off of nearby states to make value updates.
    </p>

    <div class="eq-grid" style="grid-gap: 8px; margin-top: 0; margin-bottom: 0;">
    
      <div style="grid-row: 1; grid-column: 1;"><d-math>V(s_t)~~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>\hookleftarrow~~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>r_t </d-math></div>
      <div style="grid-row: 1; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 1; grid-column: 9;"><d-math>\gamma V(s_{t+1})</d-math></div>

      <figcaption style="grid-row: 2; grid-column: 1; max-width:140px;"> State value </figcaption>
      <figcaption style="grid-row: 2; grid-column: 5; max-width:140px;">Reward</figcaption>
      <figcaption style="grid-row: 2; grid-column: 9; max-width:140px;">Next state value</figcaption>

    </div>

    <p>
      Under this update rule, intersections between two trajectories are handled differently. TD learning merges them so that the return flows backwards to all preceding states.
    <p>

  <figure id="cliffworld-td" class="l-body" style="margin-top: 0; width: 100%;">
    <center>
      <img style="width: 30%; margin: 5px; " src="figures/cliffworld-path1.svg"/>
      <img style="width: 30%; margin: 5px; " src="figures/cliffworld-path2.svg"/>
      <img style="width: 30%; margin: 5px; " src="figures/cliffworld-td.svg"/>
      <figcaption>

        <div class="eq-grid" style="grid-gap: 15%; width: 100%;">
          <div style="grid-row: 1; grid-column: 1;">
            Sometimes the agent reaches its goal.
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            Other times it falls off the cliff.
          </div>
          <div style="grid-row: 1; grid-column: 3;">
            TD learning merges paths where they intersect.
          </div>
        </div>
      </figcaption>

    </center>
  </figure>

    <p>
      What does it mean to "merge trajectories" in a more formal sense? Why might it be a good idea? One thing to notice is that $V(s_{t+1})$ can be written as the expectation over all of its TD updates:
    <p>


    <div class="eq-grid" style="grid-gap: 8px;">
    
      <div style="grid-row: 1; grid-column: 1;"><d-math>V(s_{t+1})~~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>\simeq~~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>\mathop{\mathbb{E}} \bigr[ r'_{t+1} ~+~ \gamma V(s'_{t+2}) \bigl] </d-math></div>

      <div style="grid-row: 3; grid-column: 3;"><d-math>\simeq~~</d-math></div>
      <div style="grid-row: 3; grid-column: 5;"><d-math>\mathop{\mathbb{E}} \bigr[ r'_{t+1} \bigl] ~~+~~ \gamma \mathop{\mathbb{E}} \bigr[ V(s'_{t+2}) \bigl] </d-math></div>

    </div>

    <p>
      We can use this to expand the TD update rule recursively:
    <p>

  <div class="eq-grid" style="grid-gap: 8px;">
  
    <div style="grid-row: 1; grid-column: 1;"><d-math>V(s_t)~</d-math></div>
    <div style="grid-row: 1; grid-column: 3;"><d-math>\hookleftarrow~</d-math></div>
    <div style="grid-row: 1; grid-column: 5;"><d-math>r_t </d-math></div>
    <div style="grid-row: 1; grid-column: 7;"><d-math>+</d-math></div>
    <div style="grid-row: 1; grid-column: 9;"><d-math>\gamma V(s_{t+1})</d-math></div>

    <div class="expansion-marker" style="grid-row: 2; grid-column: 9 / 14; "></div>

    <div style="grid-row: 3; grid-column: 1;"><d-math></d-math></div>
    <div style="grid-row: 3; grid-column: 3;"><d-math>\hookleftarrow~</d-math></div>
    <div style="grid-row: 3; grid-column: 5;"><d-math>r_t </d-math></div>
    <div style="grid-row: 3; grid-column: 7;"><d-math>+</d-math></div>
    <div style="grid-row: 3; grid-column: 9;"><d-math>\gamma \mathop{\mathbb{E}} \bigr[ r'_{t+1} \bigl]</d-math></div>
    <div style="grid-row: 3; grid-column: 11;"><d-math>+</d-math></div>
    <div style="grid-row: 3; grid-column: 13;"><d-math>\gamma^2 \mathop{\mathbb{E}} \bigr[ V(s''_{t+2}) \bigl]</d-math></div>

    <div class="expansion-marker" style="grid-row: 4; grid-column: 13 / 18; "></div>

    <div style="grid-row: 5; grid-column: 1;"><d-math></d-math></div>
    <div style="grid-row: 5; grid-column: 3;"><d-math>\hookleftarrow~</d-math></div>
    <div style="grid-row: 5; grid-column: 5;"><d-math>r_t </d-math></div>
    <div style="grid-row: 5; grid-column: 7;"><d-math>+</d-math></div>
    <div style="grid-row: 5; grid-column: 9;"><d-math>\gamma \mathop{\mathbb{E}} ~ \bigr[ r'_{t+1} \bigl]</d-math></div>
    <div style="grid-row: 5; grid-column: 11;"><d-math>+</d-math></div>
    <div style="grid-row: 5; grid-column: 13;"><d-math>\gamma^2 \mathop{\mathbb{EE}}  ~ \bigr[ r''_{t+2} \bigl]</d-math></div>
    <div style="grid-row: 5; grid-column: 15;"><d-math>+~</d-math></div>
    <div style="grid-row: 5; grid-column: 17;"><d-math>...~~</d-math></div>

</div>

    <p>
      We could keep repeating this expansion forever. It looks like Monte Carlo is a straightforward running average but TD learning is a recursive mess. How can we compare the two? More importantly <i>should</i> we compare the two?! Since the updates are so different, it feels a bit like we're comparing apples to oranges. Indeed, it's easy to think of Monte Carlo and TD learning as two entirely different approaches.
    </p>

    <p>
      But they are not so different after all. Let's rewrite the Monte Carlo update in terms of reward and place it beside the expanded TD update.
    </p>
  
  <div class="eq-grid" style="grid-gap: 8px;">




    <figcaption style="grid-row: 1; grid-column: 1/3; max-width:140px;"><b>MC update</b></figcaption>
    <div style="grid-row: 2; grid-column: 1;"><d-math>V(s_t)~</d-math></div>
    <div style="grid-row: 2; grid-column: 3;"><d-math>~\hookleftarrow~~</d-math></div>
    <div style="grid-row: 2; grid-column: 5;"><d-math>r_t</d-math></div>
    <div style="grid-row: 2; grid-column: 7;"><d-math>+~</d-math></div>
    <div style="grid-row: 2; grid-column: 9;"><d-math>\gamma ~ r_{t+1}</d-math></div>
    <div style="grid-row: 2; grid-column: 11;"><d-math>+~</d-math></div>
    <div style="grid-row: 2; grid-column: 13;"><d-math>\gamma^2 ~ r_{t+2}</d-math></div>
    <div style="grid-row: 2; grid-column: 15;"><d-math>+~</d-math></div>
    <div style="grid-row: 2; grid-column: 17;"><d-math>...</d-math></div>


    <figcaption style="grid-row: 3; grid-column: 5/7; max-width:90px; margin: 0px; padding: 0px;">Reward from present path.</figcaption>

    <figcaption style="grid-row: 3; grid-column: 9/11; max-width:125px; margin: 0px; padding: 0px;">Reward from present path.</figcaption>

    <figcaption style="grid-row: 3; grid-column: 13/14; max-width:125px; margin: 0px; padding: 0px;">Reward from present path...</figcaption>

    <figcaption style="grid-row: 4; grid-column: 1/3; max-width:140px; margin-top: 22px;"><b>TD update</b></figcaption>
    <div style="grid-row: 5; grid-column: 1;"><d-math>V(s_t)~</d-math></div>
    <div style="grid-row: 5; grid-column: 3;"><d-math>~\hookleftarrow~~</d-math></div>
    <div style="grid-row: 5; grid-column: 5;"><d-math>r_t</d-math></div>
    <div style="grid-row: 5; grid-column: 7;"><d-math>+~</d-math></div>
    <div style="grid-row: 5; grid-column: 9;"><d-math>\gamma \mathop{\mathbb{E}} ~ \bigr[ r'_{t+1} \bigl]</d-math></div>
    <div style="grid-row: 5; grid-column: 11;"><d-math>+~</d-math></div>
    <div style="grid-row: 5; grid-column: 13;"><d-math>\gamma^2 \mathop{\mathbb{EE}} ~ \bigr[ r''_{t+2} \bigl]</d-math></div>
    <div style="grid-row: 5; grid-column: 15;"><d-math>+~</d-math></div>
    <div style="grid-row: 5; grid-column: 17;"><d-math>...</d-math></div>

    <figcaption style="grid-row: 6; grid-column: 5/7; max-width:90px; margin: 0px; padding: 0px;">Reward from present path.</figcaption>

    <figcaption style="grid-row: 6; grid-column: 9/11; max-width:125px; margin: 0px; padding: 0px;">Expectation over paths intersecting present path.</figcaption>

    <figcaption style="grid-row: 6; grid-column: 13/14; max-width:125px; margin: 0px; padding: 0px;">Expectation over paths intersecting <i>paths intersecting</i> present path...</figcaption>

<!--
    <figcaption style="grid-row: 6; grid-column: 9/11; max-width:125px; margin: 0px; padding: 0px;">TD averages the reward of <i>all</i> trajectories passing through <d-math>s_{t+1}</d-math></figcaption>
  -->

  </div>


    <p>
      A pleasant correspondence has emerged! The difference between Monte Carlo and TD learning comes down to the nested expectation operators in the equation above. It turns out that there is a nice visual interpretation for what they are doing. We call it that <i>paths perspective</i> on value learning.
    </p>

    <h2>The Paths Perspective</h2>

    <p>
      We often think about an agent's experience as a series of trajectories. The grouping is logical and easy to visualize.
    </p>

    <figure id="cliffworld-trajectories" class="l-page" style="margin-top: 0; width: 100%;">
      <center>
        <img style="width: 20%; margin: 15px; " src="figures/cliffworld-traj-1of2.svg"/>
        <img style="width: 20%; margin: 15px; " src="figures/cliffworld-traj-2of2.svg"/>
      
      <figcaption class="l-body">
        <div class="eq-grid" style="grid-gap: 50%; width: 30%;">
          <div style="grid-row: 1; grid-column: 1;">
            Trajectory 1
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            Trajectory 2
          </div>
        </div>
      </figcaption>
      </center>
    </figure>

    <p>
      But this way of organizing experience de-emphasizes relationships <i>between</i> trajectories. Wherever two trajectories intersect, both outcomes are valid futures for the agent. So even if the agent has followed Trajectory 1 to the intersection, it could <i>in theory</i> follow Trajectory 2 from that point onward. We can dramatically expand the agent's experience using these synthetic "paths."
    </p>

        <figure id="cliffworld-mc" class="l-page" style="margin-top: 0; width: 100%;">
      <center>
        <img style="width: 20%; margin: 15px; " src="figures/cliffworld-path-1of4.svg"/>
        <img style="width: 20%; margin: 15px; " src="figures/cliffworld-path-2of4.svg"/>
        <img style="width: 20%; margin: 15px; " src="figures/cliffworld-path-3of4.svg"/>
        <img style="width: 20%; margin: 15px; " src="figures/cliffworld-path-4of4.svg"/>
      <figcaption class="l-body">

        <div class="eq-grid" style="grid-gap: 26%; width: 70%;">
          <div style="grid-row: 1; grid-column: 1;">
            Path 1
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            Path 2
          </div>
          <div style="grid-row: 1; grid-column: 3;">
            Path 3
          </div>
          <div style="grid-row: 1; grid-column: 4;">
            Path 4
          </div>
        </div>

      </figcaption>
      </center>
    </figure>

    <p>
      <b>Estimating value.</b> It turns out that Monte Carlo is averaging over trajectories whereas TD learning is averaging over paths. The nested expectation values we saw earlier correspond to the agent averaging across <i>all possible future paths</i>.
    </p>

    <p>
      <b>TODO:</b> Make this next figure interactive. User can toggle 2 trajectories and 4 paths on and off to see them add together (as transparencies). They will add together to give Monte Carlo and TD value updates respectively.</i>.
    </p>

        <figure id="cliffworld-trajectories" class="l-page" style="margin-top: 0; width: 100%;">
      <center>
        <img style="width: 20%; margin: 15px; " src="figures/cliffworld-mc.svg"/>
        <img style="width: 20%; margin: 15px; " src="figures/cliffworld-td.svg"/>
      
      <figcaption class="l-body">
        <div class="eq-grid" style="grid-gap: 30%; width: 40%;">
          <div style="grid-row: 1; grid-column: 1;">
            Monte Carlo value estimate.
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            TD value estimate.
          </div>
        </div>
      </figcaption>
      </center>
    </figure>

    <p>
      <b>Comparing the two.</b> Generally speaking, the best value estimate is the one with the lowest variance. Since both TD and Monte Carlo are empirical averages,<d-footnote>In the tabular case.</d-footnote> the method that gives the better estimate is the one that averages over more items. This raises a natural question: Which estimator averages over more items?
    </p>

    <div class="eq-grid" style="grid-gap: 8px;">
    
      <div style="grid-row: 1; grid-column: 1;"><d-math>Var[V(s)]~~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>\propto~~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>\frac{1}{N} </d-math></div>

      <figcaption style="grid-row: 2; grid-column: 1; max-width:140px;"> Variance of estimate </figcaption>
      <figcaption style="grid-row: 2; grid-column: 5; max-width:140px;">Inverse of the number of items in the average</figcaption>

    </div>

    <p>
      So far, we have seen that TD learning never averages over <i>fewer</i> trajectories than Monte Carlo. On the other hand, it can often take advantage of synthetic trajectories in order to average over <i>more</i> trajectories. Thus it seems likely that TD estimates will have lower variance.
    </p>

    <p>
      <b>Analyzing variance.</b> It's possible to express this idea more formally by writing out the variances of the Monte Carlo and TD estimates. We show how to do this in the Appendix, but for now, we'll simply show the final equations. Note that we've introduced a new piece of notation, $T(s)$, which denotes the number of trajectories that contain state $s$.
    </p>

    <div class="eq-grid" style="grid-gap: 8px;">

      <figcaption style="grid-row: 1; grid-column: 1; max-width:140px;"><b>MC variance</b></figcaption>

      <div style="grid-row: 2; grid-column: 1;"><d-math>Var[ V(s_t)]~</d-math></div>
      <div style="grid-row: 2; grid-column: 3;"><d-math>~\propto~~</d-math></div>
      <div style="grid-row: 2; grid-column: 5;"><d-math>\frac{1}{T(s_t)}</d-math></div>
      <div style="grid-row: 2; grid-column: 7;"><d-math>~+~</d-math></div>
      <div style="grid-row: 2; grid-column: 9;"><d-math>~\frac{\gamma}{T(s_{t+1})}</d-math></div>
      <div style="grid-row: 2; grid-column: 11;"><d-math>~+~</d-math></div>
      <div style="grid-row: 2; grid-column: 13;"><d-math>~~\frac{\gamma^2}{T(s_{t+2})}</d-math></div>
      <div style="grid-row: 2; grid-column: 15;"><d-math>~+~</d-math></div>
      <div style="grid-row: 2; grid-column: 17;"><d-math>...</d-math></div>


      <figcaption style="grid-row: 4; grid-column: 5;"><d-math>~~~~=</d-math></figcaption>
      <figcaption style="grid-row: 4; grid-column: 9;"><d-math>~~~~~~~~\geq</d-math></figcaption>
      <figcaption style="grid-row: 4; grid-column: 13;"><d-math>~~~~~~~~~\geq</d-math></figcaption>
      <figcaption style="grid-row: 4; grid-column: 17;"><d-math>\geq</d-math></figcaption>

      <figcaption style="grid-row: 6; grid-column: 0; max-width:140px;"><b>TD variance</b></figcaption>

      <div style="grid-row: 7; grid-column: 1;"><d-math>Var[V(s_t)]~~</d-math></div>
      <div style="grid-row: 7; grid-column: 3;"><d-math>~\propto~~</d-math></div>
      <div style="grid-row: 7; grid-column: 5;"><d-math>\frac{1}{T(s_t)}</d-math></div>
      <div style="grid-row: 7; grid-column: 7;"><d-math>~+~</d-math></div>
      <div style="grid-row: 7; grid-column: 9;"><d-math> \frac{\gamma}{ \mathop{\mathbb{E}} [ T(s'_{t+1}) ] }</d-math></div>
      <div style="grid-row: 7; grid-column: 11;"><d-math>~+~</d-math></div>
      <div style="grid-row: 7; grid-column: 13;"><d-math>\frac{\gamma^2 }{\mathop{\mathbb{EE}} [T(s''_{t+2})]}</d-math></div>
      <div style="grid-row: 7; grid-column: 15;"><d-math>~+~</d-math></div>
      <div style="grid-row: 7; grid-column: 17;"><d-math>...</d-math></div>
  </div>

    <p>
      Since every term in the first sum is greater than or equal to the corresponding term in the second sum, tabular<d-footnote>It's important to note that this inequality only holds for tabular value estimators. Later on we'll see that things change when we add function approximation to the mix.</d-footnote> TD estimates will always have equal or lower variance than MC estimates.
    </p>


    <h2>$Q$-functions and Off-policy Learning</h2>

    <p>
      There are many situations where we'd like to first estimate value and then use it to improve a policy (e.g. in policy-value iteration). It's hard to do this with $V(s)$ because it doesn't tell us anything about how to compare different actions.
    </p>

    <p>
      <b>$Q$-functions.</b> The solution is to estimate value over both states <i>and</i> actions using the function $Q(s,a)$. Now, we can select actions according to:
    </p>

    <div class="eq-grid" style="grid-gap: 8px;">
      <div style="grid-row: 1; grid-column: 1;"><d-math>a^*~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>=~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>\mathop{\textrm{argmax}} \limits_{a}</d-math></div>
      <div style="grid-row: 1; grid-column: 7;"><d-math>Q(s,a)</d-math></div>

      <figcaption style="grid-row: 2; grid-column: 1/5;">Best action</figcaption>
      <figcaption style="grid-row: 2; grid-column: 7;">$Q$-function</figcaption>
    </div>

    <p>
      Now we can select the optimal policy just by computing $a^*$ across all states. Another nice property of $Q$-functions is that we can use them to recover $V(s)$ with a simple weighted sum.
    </p>

    <figure id="qfunc-intro" class="l-page" style="width: 100%; margin-top: 15px;">
      <center>
        <img style="width: 80%; margin: 0px; " src="figures/qfunc-intro.svg"/>
          
      <figcaption class="l-body">
        <div class="eq-grid" style="grid-gap: 20%; width: 70%;">
          <div style="grid-row: 1; grid-column: 1;">
            $Q(s,a)$ estimates the value of action $a$ when in state $s$.
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            Weighted sum
          </div>
          <div style="grid-row: 1; grid-column: 3;">
            $V(s)$ estimates the value of state $s$.
          </div>
        </div>
      </figcaption>
      </center>
    </figure>

    <p>
      It's worth noting that the value function we recover is slightly different from the original value function. And not only is it different, it's better.
    </p>

    <p>
      The key is that there is often a difference between an agent's empirical distribution of actions and its actual policy distribution. What we've done by binning empirical returns by actions and then re-weighting their averages is to correct for that difference. This correction gives a better value estimate of the underlying policy.
    </p>

    <p>
      <b>Off-policy value estimation.</b> We can also use re-weighting to estimate value under a different policy than the one that collected the experience. We call this <i>off-policy</i> value estimation since the policy isn't run through the environment.
    </p>

    <p>
      A natural question about off-policy value estimation is, "What are we re-weighting over?" It's easy to say that we are re-weighting $Q$-values, but that's not the whole story. Really, we are re-weighting the agent's experience -- on which the $Q$-values were built -- to account for some of the paths being more or less likely under the new policy. It's perhaps more appropriate to say that we are <i>re-weighting over paths</i>.
    </p>

    <p>
      <b>$Q$-learning.</b> Estimating the value function of an <i>optimal</i> policy is often of special interest. Since an optimal policy function is an $\textrm{argmax}$ over the $Q$-values of each state, it makes sense to re-weight the value estimate accordingly. This is the idea behind $Q$-learning, which is actually just a special case of off-policy TD value estimation.
    </p>

    <div class="eq-grid" style="grid-gap: 8px;">
    
      <div style="grid-row: 1; grid-column: 1;"><d-math>Q(s_t, a_t)~~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>\hookleftarrow~~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>r_t </d-math></div>
      <div style="grid-row: 1; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 1; grid-column: 9;"><d-math>\gamma V(s_{t+1})</d-math></div>

      <figcaption style="grid-row: 2; grid-column: 1; max-width:140px;"> $Q$-function </figcaption>
      <figcaption style="grid-row: 2; grid-column: 5; max-width:140px;">Reward</figcaption>
      <figcaption style="grid-row: 2; grid-column: 9; max-width:140px;">Next state value</figcaption>

      <div style="grid-row: 5; grid-column: 1;"><d-math>V(s)~~</d-math></div>
      <div style="grid-row: 5; grid-column: 3;"><d-math>\hookleftarrow~~</d-math></div>
      <div style="grid-row: 5; grid-column: 5;"><d-math>\mathop{\textrm{max}} \limits_{a} Q(s,a)</d-math></div>

      <figcaption style="grid-row: 6; grid-column: 1/4; max-width:200px;"> Value function </figcaption>
      <figcaption style="grid-row: 6; grid-column: 5; max-width:140px;">Re-weighting</figcaption>

    </div>

    By reweighting paths with binary probabilities, $Q$-learning aggressively prunes the agent's experience, keeping only the paths that lead to the highest return.


    <h2>Merging Paths with Function Approximators</h2>

    <p>
      Up until now, we've kept track of a one parameter – the value of the running average – for every state. This worked fine for the simple Cliff World example, but it clearly won't work for environments that have large or infinite state-spaces. Unfortunately, most interesting RL problems fall under this category.
    </p>

    <p>
      In this section, we'll tackle value estimation when there are more states than learnable parameters. This means that, on average, updating one parameter will update more than one value estimate. There are countless methods for doing this: we refer to them collectively as <i>function approximators</i>.
    </p>

    <p>
      To get a better idea of how function approximators change things, let's return to the Cliff World example. This time, though, the environment has a large number of states.
    </p>

    <figure id="q-learning-intro" class="l-page" style="width: 100%; margin-top: 0px;">
      <center>
        <img style="width: 30%; margin: 15px; " src="figures/large-cliffworld-states.svg"/>
        <img style="width: 30%; margin: 15px; " src="figures/large-cliffworld-path.svg"/>
        <img style="width: 30%; margin: 15px; " src="figures/large-cliffworld-approx.svg"/>
          
      <figcaption class="l-body">
        <div class="eq-grid" style="grid-gap: 15%; width: 90%;">
          <div style="grid-row: 1; grid-column: 1;">
            As RL environments get bigger, the number of states grows dramatically.
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            <b>Tabular value functions</b> keep value estimates for each individual state. They consume a great deal of memory and don't generalize
          </div>
          <div style="grid-row: 1; grid-column: 3;">
            <b>Euclidean averagers</b> – which are function approximators – save memory and let agents generalize to states they haven't visited.
          </div>
        </div>
      </figcaption>
      </center>
    </figure>

    <p>
      <b>Merging nearby paths.</b> From the paths perspective, we can interpret function approximation as a way of merging nearby paths. But what do we mean by "nearby"? In the figure above, we have made an implicit decision to measure "nearby" with Euclidean distance. This was a good idea, because the Euclidean distance between two states is highly correlated with the probability that the agent will transition between them.
    </p>

    <p>
      However, it's easy to imagine cases where this implicit assumption breaks down. By adding a single long barrier, we can construct a case where the Euclidean distance metric leads to bad generalization. The problem is that we have merged the wrong paths.
    </p>

    <figure id="q-learning-intro" class="l-page" style="width: 100%; margin-top: 0px;">
      <center>
        <img style="width: 30%; margin: 15px; " src="figures/large-cliffworld-barrier-intro.svg"/>
        <img style="width: 30%; margin: 15px; " src="figures/large-cliffworld-barrier.svg"/>
          
      <figcaption class="l-body">
        <div class="eq-grid" style="grid-gap: 15%; width: 70%;">
          <div style="grid-row: 1; grid-column: 1;">
            Imagine changing the Cliff World setup by adding a long barrier.
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            Now the Euclidean distance metric gives bad generalization.
          </div>
        </div>
      </figcaption>
      </center>
    </figure>

    <p>
      <b>Merging the wrong paths.</b> The diagram below shows the ill effects of merging the wrong paths a bit more explicitly. Since the Euclidean averager is to blame for poor generalization, both Monte Carlo and TD make bad value updates. However, TD learning amplifies these errors dramatically whereas Monte Carlo does not.
    </p>

    <figure class="l-page" style="width: 100%; margin-top: 15px;">
      <center>
        <img style="width: 70%; margin: 15px; " src="figures/compare-function-approx.svg"/>
        <figcaption class="l-body" style="width: 60%;">
          Effects of state approximation when the wrong paths are merged. Bad generalization can cause more severe errors under TD learning.
        </figcaption>
      </center>
    </figure>

    <p>
      This is the price we must pay for using TD learning: it's far more sensitive to bad value updates.
    </p>
  


    <h2>Implications for deep reinforcement learning</h2>

    <p>
      <b>Neural networks.</b> One of the most popular types of function approximator, especially in recent years, has been deep neural networks. These models are exciting for many reasons, but one particularly nice property is that they don't make implicit assumptions about which states are "nearby."
    </p>

    <p>
      Early in training, neural networks, like averagers, tend to merge the wrong paths of experience. In the Cliff Walking example, an untrained neural network might make the same bad value updates as the Euclidean averager.
    </p>

    <p>
      But as training progresses, neural networks can actually learn to overcome these errors. They learn which states are "nearby" from experience. In the Cliff World example, we might expect a fully-trained neural network to have learned that value updates to states <i>above</i> the barrier should never affect the values of states <i>below</i> the barrier. This isn't something that most other function approximators can do. It's part of the reason why deep RL is so interesting!
    </p>

    <figure class="l-page" style="width: 100%; margin-top: 15px;">
      <center>
        <img style="width: 80%; margin: 15px; " src="figures/latent-distance.png"/>
      <figcaption class="l-body">
        <div class="eq-grid" style="grid-gap: 3%; width: 85%;">
          <div style="grid-row: 1; grid-column: 1;">
            A robotic hand, controlled by a neural network, that has learned its own concept of which states are nearby. <d-cite key="Srinivas2018UniversalNetworks"></d-cite>
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            Lighter blue $\rightarrow$ more distant.
          </div>
          <div style="grid-row: 1; grid-column: 3;">
            Notice that the shape and location of obstacles affects the agent's notion of distance.
          </div>
        </div>
      </figcaption>
      </center>
    </figure>

    <p>
      <b>The best of both worlds?</b> So far, we have seen that TD learning has a clear advantage over Monte Carlo in tabular settings. But for different reasons, Monte Carlo can sometimes beat TD learning (or at least compete with it) when we use function approximation <d-cite key="Amiranashvili2018TDLearning"></d-cite>. Even though neural networks can learn to overcome bad generalization, it would be better if we were able to reduce bad generalization in the first place.
    </p>

    <p>
      That's the reasoning behind TD($\lambda$) learning. It's a technique that simply interpolates (using the coefficient $\lambda$) between Monte Carlo and TD updates. <d-footnote>In the limit $\lambda=0$, we recover the TD update rule. Meanwhile, when $\lambda=1$, we recover Monte Carlo.</d-footnote> Often, TD($\lambda$) works better than either Monte Carlo or TD learning alone. <b>TODO: add simple interactive figure where we can interpolate.</b>

    <p>
      Researchers usually keep the $\lambda$ coefficient constant as they train a deep RL model. However, if Monte Carlo learning is best early in training (before the agent has learned a good state representation) and TD learning is best later on (when it's easier to benefit from merging paths), maybe we should gradually decrease $\lambda$ over the course of training...
    </p>

    <p>
      Perhaps there are other ways for deep RL agents to combine the the stability of Monte Carlo with the efficiency of TD learning. Deep RL is still a young area of research and there are many open questions. We hope this post clarifies the ideas around value estimation and inspires you to try some experiments of your own. <b>TODO: make ending stronger.</b>
    </p>



    <h2>Playground</h2>
    <br>

    <p>
      In order to build intuition about how RL works, in particular about the relationship between various types of value learning, we encourage you to try training your own Gridworld agent in the playground below.
    </p>

    <div id="playground"></div>
    <script src="vis/Playground.js" charset="utf-8"></script>




</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    Thanks to these people for advice and feedback on early versions. Thanks to the Google AI Residency Program for supporting me on this project. Some other thank-you's.
  </p>

  <h3>Variance Analysis</h3>

    <p>
      This section shows a quick derivation of the variance equations presented in the <i>Paths Perspective</i> section. As a broad overview, we will start with the expanded update rules we wrote down <i>Beating Monte Carlo</i> section. Next, we will analyze the variance of each individual term. Finally, we will write an expression for the variance of the MC and TD <i>estimates</i> using the fact that both estimates are just averages over one or more updates.
    </p>

    <p>
      Let's start with Monte Carlo. We'll also need to introduce a new piece of notation: <d-math>T(s)</d-math>, which denotes the number of trajectories that pass through state <d-math>s</d-math>.
    </p>

    <div class="eq-grid" style="grid-gap: 8px;">

      <figcaption style="grid-row: 1; grid-column: 1/5; max-width:140px;">MC update</figcaption>

      <div style="grid-row: 2; grid-column: 1;"><d-math>U(s_t)~</d-math></div>
      <div style="grid-row: 2; grid-column: 3;"><d-math>=~</d-math></div>
      <div style="grid-row: 2; grid-column: 5;"><d-math>~r_t</d-math></div>
      <div style="grid-row: 2; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 2; grid-column: 9;"><d-math>\gamma r'_{t+1}</d-math></div>
      <div style="grid-row: 2; grid-column: 11;"><d-math>+</d-math></div>
      <div style="grid-row: 2; grid-column: 13;"><d-math>\gamma^2 r''_{t+2}</d-math></div>
      <div style="grid-row: 2; grid-column: 15;"><d-math>+</d-math></div>
      <div style="grid-row: 2; grid-column: 17;"><d-math>...</d-math></div>

      <figcaption style="grid-row: 4; grid-column: 1/5; max-width:140px;">MC update variance</figcaption>

      <div style="grid-row: 5; grid-column: 1;"><d-math>Var[ U(s_t)]~</d-math></div>
      <div style="grid-row: 5; grid-column: 3;"><d-math>\propto~</d-math></div>
      <div style="grid-row: 5; grid-column: 5;"><d-math>~1</d-math></div>
      <div style="grid-row: 5; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 5; grid-column: 9;"><d-math>\gamma</d-math></div>
      <div style="grid-row: 5; grid-column: 11;"><d-math>+</d-math></div>
      <div style="grid-row: 5; grid-column: 13;"><d-math>\gamma^2</d-math></div>
      <div style="grid-row: 5; grid-column: 15;"><d-math>+</d-math></div>
      <div style="grid-row: 5; grid-column: 17;"><d-math>...</d-math></div>

      <figcaption style="grid-row: 7; grid-column: 1/5; max-width:140px;">MC variance</figcaption>

      <div style="grid-row: 8; grid-column: 1;"><d-math>Var[ V(s_t)]~</d-math></div>
      <div style="grid-row: 8; grid-column: 3;"><d-math>\propto~</d-math></div>
      <div style="grid-row: 8; grid-column: 5;"><d-math>\frac{1}{T_s}</d-math></div>
      <div style="grid-row: 8; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 8; grid-column: 9;"><d-math> \frac{\gamma}{T_s}</d-math></div>
      <div style="grid-row: 8; grid-column: 11;"><d-math>+</d-math></div>
      <div style="grid-row: 8; grid-column: 13;"><d-math>\frac{\gamma^2 }{T_s}</d-math></div>
      <div style="grid-row: 8; grid-column: 15;"><d-math>+</d-math></div>
      <div style="grid-row: 8; grid-column: 17;"><d-math>...</d-math></div>

    </div>

    <p>
      Now let's do the same exact thing for TD learning.
    </p>

    <div class="eq-grid" style="grid-gap: 8px;">

      <figcaption style="grid-row: 1; grid-column: 1/5; max-width:140px;">TD update</figcaption>

      <div style="grid-row: 2; grid-column: 1;"><d-math>U(s_t)~</d-math></div>
      <div style="grid-row: 2; grid-column: 3;"><d-math>=~</d-math></div>
      <div style="grid-row: 2; grid-column: 5;"><d-math>~r_t</d-math></div>
      <div style="grid-row: 2; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 2; grid-column: 9;"><d-math>\gamma \mathop{\mathbb{E}} [ r'_{t+1} ]</d-math></div>
      <div style="grid-row: 2; grid-column: 11;"><d-math>+</d-math></div>
      <div style="grid-row: 2; grid-column: 13;"><d-math>\gamma^2 \mathop{\mathbb{EE}} [ r''_{t+2} ]</d-math></div>
      <div style="grid-row: 2; grid-column: 15;"><d-math>+</d-math></div>
      <div style="grid-row: 2; grid-column: 17;"><d-math>...</d-math></div>

      <figcaption style="grid-row: 4; grid-column: 1/5; max-width:140px;">TD update variance</figcaption>

      <div style="grid-row: 5; grid-column: 1;"><d-math>Var[ U(s_t)]~</d-math></div>
      <div style="grid-row: 5; grid-column: 3;"><d-math>\propto~</d-math></div>
      <div style="grid-row: 5; grid-column: 5;"><d-math>~1</d-math></div>
      <div style="grid-row: 5; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 5; grid-column: 9;"><d-math>\frac{\gamma}{ \mathop{\mathbb{E}} [ T_{s'_{t+1}} ] }</d-math></div>
      <div style="grid-row: 5; grid-column: 11;"><d-math>+</d-math></div>
      <div style="grid-row: 5; grid-column: 13;"><d-math>\frac{\gamma^2}{ \mathop{\mathbb{EE}} [T_{s''_{t+2}}]}</d-math></div>
      <div style="grid-row: 5; grid-column: 15;"><d-math>+</d-math></div>
      <div style="grid-row: 5; grid-column: 17;"><d-math>...</d-math></div>

      <figcaption style="grid-row: 7; grid-column: 1/5; max-width:140px;">TD variance</figcaption>

      <div style="grid-row: 8; grid-column: 1;"><d-math>Var[ V(s_t)]~~</d-math></div>
      <div style="grid-row: 8; grid-column: 3;"><d-math>~\propto~~</d-math></div>
      <div style="grid-row: 8; grid-column: 5;"><d-math>\frac{1}{T_s}</d-math></div>
      <div style="grid-row: 8; grid-column: 7;"><d-math>~+~</d-math></div>
      <div style="grid-row: 8; grid-column: 9;"><d-math> \frac{\gamma}{ \mathop{\mathbb{E}} [ T(s'_{t+1}) ]} </d-math></div>
      <div style="grid-row: 8; grid-column: 11;"><d-math>~+~</d-math></div>
      <div style="grid-row: 8; grid-column: 13;"><d-math>\frac{\gamma^2 }{\mathop{\mathbb{EE}} [T(s''_{t+2})]}</d-math></div>
      <div style="grid-row: 8; grid-column: 15;"><d-math>~+~</d-math></div>
      <div style="grid-row: 8; grid-column: 17;"><d-math>...</d-math></div>

    </div>


    <p>
      Now, if we rewrite the third line of the Monte Carlo analysis alongside the third line of the TD analysis, we can get the same expression that we wrote down at the end of the <i>Paths Perspective</i> section:
    </p>

    <div class="eq-grid" style="grid-gap: 8px;">

      <figcaption style="grid-row: 1; grid-column: 1; max-width:140px;">MC variance</figcaption>

      <div style="grid-row: 2; grid-column: 1;"><d-math>Var[ V(s_t)]~</d-math></div>
      <div style="grid-row: 2; grid-column: 3;"><d-math>~\propto~~</d-math></div>
      <div style="grid-row: 2; grid-column: 5;"><d-math>\frac{1}{T(s_t)}</d-math></div>
      <div style="grid-row: 2; grid-column: 7;"><d-math>~+~</d-math></div>
      <div style="grid-row: 2; grid-column: 9;"><d-math>~\frac{\gamma}{T(s_{t+1})}</d-math></div>
      <div style="grid-row: 2; grid-column: 11;"><d-math>~+~</d-math></div>
      <div style="grid-row: 2; grid-column: 13;"><d-math>~~\frac{\gamma^2}{T(s_{t+2})}</d-math></div>
      <div style="grid-row: 2; grid-column: 15;"><d-math>~+~</d-math></div>
      <div style="grid-row: 2; grid-column: 17;"><d-math>...</d-math></div>


      <figcaption style="grid-row: 4; grid-column: 5;"><d-math>~~~~=</d-math></figcaption>
      <figcaption style="grid-row: 4; grid-column: 9;"><d-math>~~~~~~~~\geq</d-math></figcaption>
      <figcaption style="grid-row: 4; grid-column: 13;"><d-math>~~~~~~~~~\geq</d-math></figcaption>
      <figcaption style="grid-row: 4; grid-column: 17;"><d-math>\geq</d-math></figcaption>

      <figcaption style="grid-row: 5; grid-column: 0; max-width:140px;">TD variance</figcaption>

      <div style="grid-row: 6; grid-column: 1;"><d-math>Var[V(s_t)]~~</d-math></div>
      <div style="grid-row: 6; grid-column: 3;"><d-math>~\propto~~</d-math></div>
      <div style="grid-row: 6; grid-column: 5;"><d-math>\frac{1}{T(s_t)}</d-math></div>
      <div style="grid-row: 6; grid-column: 7;"><d-math>~+~</d-math></div>
      <div style="grid-row: 6; grid-column: 9;"><d-math> \frac{\gamma}{ \mathop{\mathbb{E}} [ T(s'_{t+1}) ] }</d-math></div>
      <div style="grid-row: 6; grid-column: 11;"><d-math>~+~</d-math></div>
      <div style="grid-row: 6; grid-column: 13;"><d-math>\frac{\gamma^2 }{\mathop{\mathbb{E}} \mathop{\mathbb{E}} [T(s''_{t+2})]}</d-math></div>
      <div style="grid-row: 6; grid-column: 15;"><d-math>~+~</d-math></div>
      <div style="grid-row: 6; grid-column: 17;"><d-math>...</d-math></div>
    </div>

    <p>
      The next step is to note the 1:1 correspondence of terms in the two sums. For all <d-math>t' > t</d-math>we can say that <d-math>\mathop{\mathbb{E}} [T(s_{t'})] \geq T(s_{t'}) </d-math>. This means that the variance of the TD estimate will be equal or lower than the Monte Carlo estimate.
    </p>


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
