<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
  <style type="text/css">@import "vis/hero.css";</style>
  
  <script src="vis/js/d3.min.js" charset="utf-8"></script>
  <script src="vis/js/underscore.js" charset="utf-8"></script>
  <script src="vis/js/rounding.js" charset="utf-8"></script>
  <script src="vis/util.js" charset="utf-8"></script>
  <script src="vis/GridWorld.js" charset="utf-8"></script>
  <script src="vis/LearnQV.js" charset="utf-8"></script>
  <script src="vis/Policy.js" charset="utf-8"></script>
  <script src="vis/Aprox.js" charset="utf-8"></script>

  <script src="vis/cliff.js" charset="utf-8"></script>
  <script src="vis/compare.js" charset="utf-8"></script>
  <script src="vis/tug.js" charset="utf-8"></script>
  <script src="vis/tug_baseline.js" charset="utf-8"></script>
  <script src="vis/reinforce.js" charset="utf-8"></script>

</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "The Paths Perspective on Value Learning",
  "description": "",
  "password": "td",
  "authors": [

    {
      "author": "Chris Olah",
      "authorURL": "https://colah.github.io/",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    },

    {
      "author": "Sam Greydanus",
      "authorURL": "https://greydanus.github.io/about.html",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    },

    {
      "author": "Justin Gilmer",
      "authorURL": "https://scholar.google.com/citations?user=Ml_vQ8MAAAAJ&hl=en",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    }
  ],
  "katex": {
    "strict": false,
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<style>
  #cliff_iter .cell rect {
    fill: #e7eae7;
  }
  .cell rect {
    fill: #e7eae7;
  }
</style>

<d-title>
  <h1>The Paths Perspective on Value Learning</h1>
  <p>
    A closer look at how Temporal Difference (TD) learning merges paths of experience for greater statistical efficiency.
  </p>
</d-title>

  <script>
    function do_hero_vis(run) {
      compare_vis(d3.select("#compare_hero"), {algs: ["MC", "TD", "Q"] }, run);
    }
  </script>

  <figure>
    <div id="compare_hero" style="width:1000px; height:350px; margin:auto; position:relative;"></div>
    <script> do_hero_vis(false); </script> 
  </figure>

  <center> <button type="button" onclick='do_hero_vis(true)'>Run</button> </center>

<d-article>
<!-- <d-cite key="Tsividis2017HumanAtari"></d-cite> -->
  
  <h2>Introduction</h2>
  <p>
    In the last few years, reinforcement learning (RL) has made remarkable progress, including <a href="https://deepmind.com/research/alphago/">beating world-champion Go players</a>, <a href="https://blog.openai.com/learning-dexterity/">controlling robotic hands</a>, and even <a href="https://deepmind.com/blog/learning-to-generate-images/">painting pictures</a>.
  </p>

  <p>
    One of the key sub-problems of RL is value estimation – learning the long-term consequences of being in a state.

    This can be tricky because future returns are generally noisy, affected by many things other than the present state. The further we look into the future, the more this becomes true.

    But while difficult, estimating value is also essential to most approaches to RL.<d-footnote>For many approaches (policy-value iteration), estimating value essentially is the whole problem, while in other approaches (actor-critic models), value estimation is essential for reducing noise.</d-footnote>
  </p>

  <p>
    The natural way to estimate the value of a state is as the average return you observe from that state. We call this Monte Carlo value estimation. 
  </p>




  <figure id="cliffworld-mc" class="l-body" style="margin-top: 0; width: 100%;">

      <img style="width: 30%; margin: 1%; " src="figures/cliffworld-path1.svg"/>
      <img style="width: 30%; margin: 1%; " src="figures/cliffworld-path2.svg"/>
      <img style="width: 30%; margin: 1%; " src="figures/cliffworld-mc.svg"/>

      <figcaption>
        <div class="eq-grid" style="grid-gap: 20%; width: 100%; float: right;">
          <figcaption style="grid-row: 1; grid-column: 1;">
            Sometimes the agent reaches its goal.
          </figcaption>
          <figcaption style="grid-row: 1; grid-column: 2;">
            Other times it falls off the cliff.
          </figcaption>
          <figcaption style="grid-row: 1; grid-column: 3;">
            Monte Carlo averages over the two trajectories.
          </figcaption>
        </div>
      </figcaption>
  </figure>

  <figcaption class="l-gutter">
    <b>Cliff World</b> <d-cite key="?"></d-cite> is a classic RL example, where the agent learns to walk along a <span style="background: #FFD5D5">cliff</span> to reach a <span style="background: #D5D5FF">goal</span>.
  </figcaption>

  <p>
    If a state is visited by only one episode, Monte Carlo says its value is the return of that episode. If multiple episodes visit a state, Monte Carlo estimates its value to be the average over them.
  </p>

  <p>
    Let's write Monte Carlo a bit more formally.
    In RL, we often describe algorithms with update rules, which tell us how estimates change with one more episode.
    We'll use an "updates toward" ($\hookleftarrow$) operator to keep equations simple.
  </p>

  <div class="eq-grid" style="grid-gap: 8px;">
      <div style="grid-row: 1; grid-column: 1;"><d-math>V(s_t)~~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>\hookleftarrow~~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>R_t</d-math></div>

      <figcaption style="grid-row: 2; grid-column: 1; max-width:140px;"> State value </figcaption>
      <figcaption style="grid-row: 2; grid-column: 5; max-width:140px;"> Return </figcaption>
  </div>

  <p>
    Estimating value in this way makes a lot of sense. In fact, it might be surprising that we can do better.
  </p>

  <h2>Beating Monte Carlo</h2>

    <p>
      But we <i>can</i> do better! The trick is to use a method called <i>Temporal Difference (TD) learning</i> which bootstraps off of nearby states to make value updates.
    </p>

    <div class="eq-grid" style="grid-gap: 8px;">
    
      <div style="grid-row: 1; grid-column: 1;"><d-math>V(s_t)~~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>\hookleftarrow~~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>r_t </d-math></div>
      <div style="grid-row: 1; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 1; grid-column: 9;"><d-math>\gamma V(s_{t+1})</d-math></div>

      <figcaption style="grid-row: 2; grid-column: 1; max-width:140px;"> State value </figcaption>
      <figcaption style="grid-row: 2; grid-column: 5; max-width:140px;">Reward</figcaption>
      <figcaption style="grid-row: 2; grid-column: 9; max-width:140px;">Next state value</figcaption>

    </div>

    <p>
      Under this update rule, intersections between two trajectories are handled differently. TD learning merges them so that the return flows backwards to all preceding states.
    <p>

  <figure id="cliffworld-td" class="l-body" style="margin-top: 0; width: 100%;">
    <center>
      <img style="width: 30%; margin: 5px; " src="figures/cliffworld-path1.svg"/>
      <img style="width: 30%; margin: 5px; " src="figures/cliffworld-path2.svg"/>
      <img style="width: 30%; margin: 5px; " src="figures/cliffworld-td.svg"/>
      <figcaption>

        <div class="eq-grid" style="grid-gap: 15%; width: 100%;">
          <div style="grid-row: 1; grid-column: 1;">
            Sometimes the agent reaches its goal.
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            Other times it falls off the cliff.
          </div>
          <div style="grid-row: 1; grid-column: 3;">
            TD learning merges paths where they intersect.
          </div>
        </div>
      </figcaption>

    </center>
  </figure>

    <p>
      What does it mean to "merge trajectories" in a more formal sense? Why might it be a good idea? First, notice that $V(s_{t+1})$ is estimating the expected value of its updates:
    <p>


    <div class="eq-grid" style="grid-gap: 8px;">
    
      <div style="grid-row: 1; grid-column: 1;"><d-math>V(s_{t+1})~~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>\simeq~~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>\mathop{\mathbb{E}} \bigr[ r'_{t+1} ~+~ \gamma V(s'_{t+2}) \bigl] </d-math></div>

      <div style="grid-row: 3; grid-column: 3;"><d-math>\simeq~~</d-math></div>
      <div style="grid-row: 3; grid-column: 5;"><d-math>\mathop{\mathbb{E}} \bigr[ r'_{t+1} \bigl] ~~+~~ \mathop{\mathbb{E}} \bigr[ \gamma V(s'_{t+2}) \bigl] </d-math></div>

    </div>

    <p>
      We can use this to expand the recursive update rule:
    <p>

  <div class="eq-grid" style="grid-gap: 8px;">
  
    <div style="grid-row: 1; grid-column: 1;"><d-math>V(s_t)~</d-math></div>
    <div style="grid-row: 1; grid-column: 3;"><d-math>\hookleftarrow~</d-math></div>
    <div style="grid-row: 1; grid-column: 5;"><d-math>r_t </d-math></div>
    <div style="grid-row: 1; grid-column: 7;"><d-math>+</d-math></div>
    <div style="grid-row: 1; grid-column: 9;"><d-math>\gamma V(s_{t+1})</d-math></div>

    <div style="grid-row: 3; grid-column: 1;"><d-math></d-math></div>
    <div style="grid-row: 3; grid-column: 3;"><d-math>\hookleftarrow~</d-math></div>
    <div style="grid-row: 3; grid-column: 5;"><d-math>r_t </d-math></div>
    <div style="grid-row: 3; grid-column: 7;"><d-math>+</d-math></div>
    <div style="grid-row: 3; grid-column: 9;"><d-math>\gamma \mathop{\mathbb{E}} \bigr[ r'_{t+1}</d-math></div>
    <div style="grid-row: 3; grid-column: 11;"><d-math>+</d-math></div>
    <div style="grid-row: 3; grid-column: 13;"><d-math>\gamma V(s'_{t+2}) \bigl]</d-math></div>

    <div style="grid-row: 5; grid-column: 1;"><d-math></d-math></div>
    <div style="grid-row: 5; grid-column: 3;"><d-math>\hookleftarrow~</d-math></div>
    <div style="grid-row: 5; grid-column: 5;"><d-math>r_t </d-math></div>
    <div style="grid-row: 5; grid-column: 7;"><d-math>+</d-math></div>
    <div style="grid-row: 5; grid-column: 9;"><d-math>\gamma \mathop{\mathbb{E}} \bigr[ r'_{t+1} \bigl]</d-math></div>
    <div style="grid-row: 5; grid-column: 11;"><d-math>+</d-math></div>
    <div style="grid-row: 5; grid-column: 13;"><d-math>\gamma^2 \mathop{\mathbb{E}} \bigr[ V(s'_{t+2}) \bigl]</d-math></div>

    <div style="grid-row: 7; grid-column: 1;"><d-math></d-math></div>
    <div style="grid-row: 7; grid-column: 3;"><d-math>\hookleftarrow~</d-math></div>
    <div style="grid-row: 7; grid-column: 5;"><d-math>r_t </d-math></div>
    <div style="grid-row: 7; grid-column: 7;"><d-math>+</d-math></div>
    <div style="grid-row: 7; grid-column: 9;"><d-math>\gamma \mathop{\mathbb{E}} ~ \bigr[ r'_{t+1} \bigl]</d-math></div>
    <div style="grid-row: 7; grid-column: 11;"><d-math>+</d-math></div>
    <div style="grid-row: 7; grid-column: 13;"><d-math>\gamma^2 \mathop{\mathbb{E}} \mathop{\mathbb{E}} ~ \bigr[ r''_{t+2} + \gamma V(s''_{t+3})\bigl]</d-math></div>

</div>

    <p>
      We could keep repeating this expansion forever. So while the Monte Carlo update is a straightforward running average, the TD rule is a recursive mess. How can we compare the two? More importantly <i>should</i> we compare the two?! Since the updates are so different, it feels a bit like we're comparing apples to oranges. Indeed, it's easy to think of Monte Carlo and TD learning as two entirely different approaches.
    </p>

    <p>
      But they are not so different after all. Let's expand the TD learning update a bit more and then rewrite the Monte Carlo update in terms of reward rather than return.
    </p>
  
  <div class="eq-grid" style="grid-gap: 8px;">

    <div style="grid-row: 1; grid-column: 1;"><d-math>V(s_t)~</d-math></div>
    <div style="grid-row: 1; grid-column: 3;"><d-math>\hookleftarrow~</d-math></div>
    <div style="grid-row: 1; grid-column: 5;"><d-math>r_t</d-math></div>
    <div style="grid-row: 1; grid-column: 7;"><d-math>+</d-math></div>
    <div style="grid-row: 1; grid-column: 9;"><d-math>\gamma \mathop{\mathbb{E}} ~ \bigr[ r'_{t+1} \bigl]</d-math></div>
    <div style="grid-row: 1; grid-column: 11;"><d-math>+</d-math></div>
    <div style="grid-row: 1; grid-column: 13;"><d-math>\gamma^2 \mathop{\mathbb{E}} \mathop{\mathbb{E}} ~ \bigr[ r''_{t+2} \bigl]</d-math></div>
    <div style="grid-row: 1; grid-column: 15;"><d-math>+</d-math></div>
    <div style="grid-row: 1; grid-column: 17;"><d-math>...</d-math></div>

    <figcaption style="grid-row: 2; grid-column: 1; max-width:140px;">TD update</figcaption>

    <div style="grid-row: 5; grid-column: 1;"><d-math>V(s_t)~</d-math></div>
    <div style="grid-row: 5; grid-column: 3;"><d-math>\hookleftarrow~</d-math></div>
    <div style="grid-row: 5; grid-column: 5;"><d-math>r_t</d-math></div>
    <div style="grid-row: 5; grid-column: 7;"><d-math>+</d-math></div>
    <div style="grid-row: 5; grid-column: 9;"><d-math>\gamma r_{t+1}</d-math></div>
    <div style="grid-row: 5; grid-column: 11;"><d-math>+</d-math></div>
    <div style="grid-row: 5; grid-column: 13;"><d-math>\gamma^2 r_{t+2}</d-math></div>
    <div style="grid-row: 5; grid-column: 15;"><d-math>+</d-math></div>
    <div style="grid-row: 5; grid-column: 17;"><d-math>...</d-math></div>

    <figcaption style="grid-row: 6; grid-column: 1; max-width:140px;">MC update</figcaption>

  </div>


    <p>
      A pleasant correspondence has emerged! The difference between Monte Carlo and TD learning comes down to the nested expectation operators in the equation above. It turns out that there is a nice visual interpretation for what these expectation operators are doing. We call it that <i>paths perspective</i> on value learning.
    </p>

    <h2>The Paths Perspective</h2>

    <p>
      It's easy to think about an agent's experience as a series of trajectories. The grouping is logical and easy to visualize.
    </p>

    <figure id="cliffworld-trajectories" class="l-page" style="margin-top: 0; width: 100%;">
      <center>
        <img style="width: 20%; margin: 15px; " src="figures/cliffworld-traj-1of2.svg"/>
        <img style="width: 20%; margin: 15px; " src="figures/cliffworld-traj-2of2.svg"/>
      
      <figcaption class="l-body">
        <div class="eq-grid" style="grid-gap: 50%; width: 30%;">
          <div style="grid-row: 1; grid-column: 1;">
            Trajectory 1
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            Trajectory 2
          </div>
        </div>
      </figcaption>
      </center>
    </figure>

    <p>
      But this way of organizing experience de-emphasizes relationships <i>between</i> trajectories. We need to realize that, wherever two trajectories intersect at a state, both outcomes are valid futures for the agent. So even if the agent has followed Trajectory 1 to the intersection, it could <i>in theory</i> follow Trajectory 2 from that point onward. We can dramatically expand the agent's experience using these synthetic "paths."
    </p>

        <figure id="cliffworld-mc" class="l-page" style="margin-top: 0; width: 100%;">
      <center>
        <img style="width: 20%; margin: 15px; " src="figures/cliffworld-path-1of4.svg"/>
        <img style="width: 20%; margin: 15px; " src="figures/cliffworld-path-2of4.svg"/>
        <img style="width: 20%; margin: 15px; " src="figures/cliffworld-path-3of4.svg"/>
        <img style="width: 20%; margin: 15px; " src="figures/cliffworld-path-4of4.svg"/>
      <figcaption class="l-body">

        <div class="eq-grid" style="grid-gap: 26%; width: 70%;">
          <div style="grid-row: 1; grid-column: 1;">
            Path 1
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            Path 2
          </div>
          <div style="grid-row: 1; grid-column: 3;">
            Path 3
          </div>
          <div style="grid-row: 1; grid-column: 4;">
            Path 4
          </div>
        </div>

      </figcaption>
      </center>
    </figure>

    <p>
      <b>Estimating value.</b> It turns out that Monte Carlo is averaging over trajectories whereas TD learning is averaging over paths. The nested expectation values we saw earlier correspond to the agent averaging across <i>all possible future paths</i>.
    </p>

        <figure id="cliffworld-trajectories" class="l-page" style="margin-top: 0; width: 100%;">
      <center>
        <img style="width: 20%; margin: 15px; " src="figures/cliffworld-mc.svg"/>
        <img style="width: 20%; margin: 15px; " src="figures/cliffworld-td.svg"/>
      
      <figcaption class="l-body">
        <div class="eq-grid" style="grid-gap: 30%; width: 40%;">
          <div style="grid-row: 1; grid-column: 1;">
            Monte Carlo value estimate.
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            TD value estimate.
          </div>
        </div>
      </figcaption>
      </center>
    </figure>

    <p>
      <b>Effect on variance.</b> Generally speaking, the best value estimate is the one with the lowest variance. Since both TD and Monte Carlo are empirical averages,<d-footnote>In the tabular case.</d-footnote> the method that gives the better estimate, on average, will be the one that averages over more items. This raises a natural question: Which estimator averages over more items?
    </p>

    <div class="eq-grid" style="grid-gap: 8px;">
    
      <div style="grid-row: 1; grid-column: 1;"><d-math>Var[V(s)]~~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>\propto~~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>\frac{1}{N} </d-math></div>

      <figcaption style="grid-row: 2; grid-column: 1; max-width:140px;"> Variance of estimate </figcaption>
      <figcaption style="grid-row: 2; grid-column: 5; max-width:140px;">Inverse of the number of items in the average</figcaption>

    </div>

    <p>
      <b>Comparing the estimators.</b> Let's start with the Monte Carlo estimator. As we've seen, it computes $V(s)$ as a simple average. This average runs over all trajectories that include $s$, which we will denote $T_s$.
    </p>

  <div class="eq-grid" style="grid-gap: 8px;">

    <div style="grid-row: 1; grid-column: 1;"><d-math>U(s_t)~</d-math></div>
    <div style="grid-row: 1; grid-column: 3;"><d-math>=~</d-math></div>
    <div style="grid-row: 1; grid-column: 5;"><d-math>r_t</d-math></div>
    <div style="grid-row: 1; grid-column: 7;"><d-math>+</d-math></div>
    <div style="grid-row: 1; grid-column: 9;"><d-math>\gamma r'_{t+1}</d-math></div>
    <div style="grid-row: 1; grid-column: 11;"><d-math>+</d-math></div>
    <div style="grid-row: 1; grid-column: 13;"><d-math>\gamma^2 r''_{t+2}</d-math></div>
    <div style="grid-row: 1; grid-column: 15;"><d-math>+</d-math></div>
    <div style="grid-row: 1; grid-column: 17;"><d-math>...</d-math></div>

    <figcaption style="grid-row: 2; grid-column: 1; max-width:140px;">MC update</figcaption>

    <div style="grid-row: 5; grid-column: 1;"><d-math>Var[ U(s_t)]^{-1}~</d-math></div>
    <div style="grid-row: 5; grid-column: 3;"><d-math>\propto~</d-math></div>
    <div style="grid-row: 5; grid-column: 5;"><d-math>1</d-math></div>
    <div style="grid-row: 5; grid-column: 7;"><d-math>+</d-math></div>
    <div style="grid-row: 5; grid-column: 9;"><d-math>\gamma 1</d-math></div>
    <div style="grid-row: 5; grid-column: 11;"><d-math>+</d-math></div>
    <div style="grid-row: 5; grid-column: 13;"><d-math>\gamma^2 1</d-math></div>
    <div style="grid-row: 5; grid-column: 15;"><d-math>+</d-math></div>
    <div style="grid-row: 5; grid-column: 17;"><d-math>...</d-math></div>

    <figcaption style="grid-row: 6; grid-column: 1; max-width:140px;">Var. of MC update</figcaption>

    <div style="grid-row: 9; grid-column: 1;"><d-math>Var[ V(s_t)]^{-1}~</d-math></div>
    <div style="grid-row: 9; grid-column: 3;"><d-math>\propto~</d-math></div>
    <div style="grid-row: 9; grid-column: 5;"><d-math>T_s</d-math></div>
    <div style="grid-row: 9; grid-column: 7;"><d-math>+</d-math></div>
    <div style="grid-row: 9; grid-column: 9;"><d-math>\gamma T_s</d-math></div>
    <div style="grid-row: 9; grid-column: 11;"><d-math>+</d-math></div>
    <div style="grid-row: 9; grid-column: 13;"><d-math>\gamma^2 T_s</d-math></div>
    <div style="grid-row: 9; grid-column: 15;"><d-math>+</d-math></div>
    <div style="grid-row: 9; grid-column: 17;"><d-math>...</d-math></div>

    <figcaption style="grid-row: 10; grid-column: 1; max-width:140px;">Var. of MC estimate</figcaption>


    <div style="grid-row: 13; grid-column: 1;"><d-math>Var[ V(s_t)]~</d-math></div>
    <div style="grid-row: 13; grid-column: 3;"><d-math>=~~</d-math></div>
    <div style="grid-row: 13; grid-column: 5;"><d-math>\frac{c}{T_s}</d-math></div>

    <figcaption style="grid-row: 14; grid-column: 1; max-width:140px;">Var. of MC estimate</figcaption>

  </div>

    <p>
      Now let's turn our attention to TD learning. The first thing to note is that when trajectories <i>don't</i> intersect, it makes the same updates as Monte Carlo. This is because the "paths" picture looks exactly like the "trajectories" picture.
    </p>

    <p>
      Whenever two trajectories do intersect, though, TD learning has the opportunity to do much better. Since TD learning averages over all future paths, it's tempting to say that its variance scales with the number of paths. This is false, though, because many paths are not be independent of each other. <d-footnote>For a concrete example of this idea, look back at the synthetic paths through Cliff World. You'll see, for example, that there are two paths that lead to the outcome where the agent reaches the goal. If we count these paths as separate items in an average, we will end up double-counting the contribution of this outcome. Hence we should count the number of <i>real</i> trajectories that are accessible via "synthetic" paths.</d-footnote>
    </p>

    <p>
      We can fix this problem by considering only the real trajectories that include the future states $s'_{t+1}, s''_{t+2}$, and so forth. Using the expanded TD update we wrote down earlier, we can show that TD estimates always have equal or lower variance than Monte Carlo estimates.
    </p>

    <div class="eq-grid" style="grid-gap: 8px;">

      <div style="grid-row: 1; grid-column: 1;"><d-math>U(s_t)~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>=~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>r_t</d-math></div>
      <div style="grid-row: 1; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 1; grid-column: 9;"><d-math>\gamma \mathop{\mathbb{E}}\limits_{s'_{t+1}} [ r'_{t+1} ]</d-math></div>
      <div style="grid-row: 1; grid-column: 11;"><d-math>+</d-math></div>
      <div style="grid-row: 1; grid-column: 13;"><d-math>\gamma^2 \mathop{\mathbb{E}}\limits_{s'_{t+1}} \mathop{\mathbb{E}}\limits_{s''_{t+2}} [ r''_{t+2} ]</d-math></div>
      <div style="grid-row: 1; grid-column: 15;"><d-math>+</d-math></div>
      <div style="grid-row: 1; grid-column: 17;"><d-math>...</d-math></div>

      <figcaption style="grid-row: 2; grid-column: 1; max-width:140px;">TD update</figcaption>

      <div style="grid-row: 5; grid-column: 1;"><d-math>Var[ U(s_t)]^{-1}~</d-math></div>
      <div style="grid-row: 5; grid-column: 3;"><d-math>\propto~</d-math></div>
      <div style="grid-row: 5; grid-column: 5;"><d-math>1</d-math></div>
      <div style="grid-row: 5; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 5; grid-column: 9;"><d-math>\gamma \frac{1}{ T_{s'_{t+1}}}</d-math></div>
      <div style="grid-row: 5; grid-column: 11;"><d-math>+</d-math></div>
      <div style="grid-row: 5; grid-column: 13;"><d-math>\gamma^2 \frac{1}{\mathop{\mathbb{E}}\limits_{s'} \mathop{\mathbb{E}}\limits_{s''} [T_{s'_{t+1}} T_{s''_{t+2}}]}</d-math></div>
      <div style="grid-row: 5; grid-column: 15;"><d-math>+</d-math></div>
      <div style="grid-row: 5; grid-column: 17;"><d-math>...</d-math></div>

      <figcaption style="grid-row: 6; grid-column: 1; max-width:140px;">Var. of TD estimate</figcaption>

      <div style="grid-row: 9; grid-column: 1;"><d-math>Var[V(s)]^{-1}~~</d-math></div>
      <div style="grid-row: 9; grid-column: 3;"><d-math>\propto~~</d-math></div>
      <div style="grid-row: 9; grid-column: 5;"><d-math>T_{s}</d-math></div>
      <div style="grid-row: 9; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 9; grid-column: 9;"><d-math>\gamma \frac{T_{s}}{\mathop{\mathbb{E}} \limits_{s'} [T_{s'_{t+1}}]}</d-math></div>
      <div style="grid-row: 9; grid-column: 11;"><d-math>+</d-math></div>
      <div style="grid-row: 9; grid-column: 13;"><d-math>\gamma^2 \frac{T_{s}}{\mathop{\mathbb{E}}\limits_{s'} \mathop{\mathbb{E}}\limits_{s''} [T_{s'_{t+1}} T_{s''_{t+2}}]}</d-math></div>
      <div style="grid-row: 9; grid-column: 15;"><d-math>+</d-math></div>
      <div style="grid-row: 9; grid-column: 17;"><d-math>...</d-math></div>

      <figcaption style="grid-row: 10; grid-column: 1; max-width:140px;">Var. of TD estimate</figcaption>

      <div style="grid-row: 13; grid-column: 1;"><d-math>Var[ V(s_t)]~</d-math></div>
      <div style="grid-row: 13; grid-column: 3;"><d-math>\leq~~</d-math></div>
      <div style="grid-row: 13; grid-column: 5;"><d-math>\frac{c}{T_s}</d-math></div>

      <figcaption style="grid-row: 14; grid-column: 1; max-width:140px;">Var. of TD estimate</figcaption>

    </div>

    <p>
      It's important to note that this inequality only holds for tabular value estimators. Later on we'll see how it breaks down when we add function approximation into the mix.
    </p>
    <p>
     First, we're going to take a quick detour through the world of $Q$-functions and off-policy learning in order to see another nice property of paths: they can be re-weighted.
   </p>


    <h2>$Q$-functions and off-policy learning</h2>

    <p>
      Until now, we have focused on the problem of value estimation alone. Many times, once we've done value estimation, we'd like to use it to choose a better policy. It's not immediately obvious how to do that 


      If an agent wants to earn the most rewards possible, then it makes sense to focus on paths with the highest value. This is the idea behind Q-learning, which is just TD learning with two tiny modifications.
    </p>

    <center><p>$Q(s_t, a_t) \hookleftarrow r_t + \gamma V(s_{t+1})$</p></center>
    <center><p>$V(s) \max_a r_t + Q(s, a)$</p></center>

    <p>
      The first thing we've done is chop the value function up into several bins. Each of these bins corresponds to one of the actions available to the agent in state $s_t$.
    <p>

    <figure id="qfunc-intro" class="l-page" style="width: 100%; margin-top: 0px;">
      <center>
        <img style="width: 80%; margin: 0px; " src="figures/qfunc-intro.svg"/>
          
      <figcaption class="l-body">
        <div class="eq-grid" style="grid-gap: 50%; width: 70%;">
          <div style="grid-row: 1; grid-column: 1;">
            $Q(s,a)$ estimates the value of taking action $a$ in state $s$.
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            $V(s)$ estimates the value of a state under the current policy.
          </div>
        </div>
      </figcaption>
      </center>
    </figure>

    <p>
      The second thing we've done is write the value of the state as the maximum value estimate across all possible actions. This makes sense, because when the agent is not exploring – when it wants to simply achieve the maximum value possible – it should choose the action which has the highest value estimate. In that case, Q-learning gives us a clear picture of what the best possible outcome looks like.
    <p>

    <p>
      <b>Overconfidence of $Q$-learning.</b> Even though $Q$-learning has some nice properties...
    </p>


    <h2>How function approximation changes things</h2>

    <p>
      Up until now, we've kept track of a one parameter – the value of the running average – for every state. This worked fine for the simple Cliff World example, but it clearly won't work for environments that have very large or even infinite state-spaces. Unfortunately, most interesting RL problems fall under this category.
    </p>

    <p>
      What we want is to learn fewer parameters than there are states. This means that, on average, updating one of these parameters will update the value estimate of more than one state. There are many ways of doing this in practice, but they all fall under the general idea of <i>function approximation</i>.
    </p>

    <p>
      To get a better idea of how function approximation changes things, let's return to the Cliff World example. This time, though, the environment has a lot more states than before. In the figure below, you can see the difference between tabular value estimation.
    </p>

    <figure id="q-learning-intro" class="l-page" style="width: 100%; margin-top: 0px;">
      <center>
        <img style="width: 30%; margin: 15px; " src="figures/large-cliffworld-states.svg"/>
        <img style="width: 30%; margin: 15px; " src="figures/large-cliffworld-path.svg"/>
        <img style="width: 30%; margin: 15px; " src="figures/large-cliffworld-approx.svg"/>
          
      <figcaption class="l-body">
        <div class="eq-grid" style="grid-gap: 15%; width: 90%;">
          <div style="grid-row: 1; grid-column: 1;">
            As RL environments get bigger, the number of states grows dramatically.
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            Tabular value functions, which keep value estimates for each individual state, consume a great deal of memory and don't generalize
          </div>
          <div style="grid-row: 1; grid-column: 3;">
            Function approximators save memory and let agents generalize to states they haven't visited.
          </div>
        </div>
      </figcaption>
      </center>
    </figure>

    <p>
      <b>Merging "nearby" paths.</b> From the paths perspective, we can interpret function approximation as a way of merging nearby paths. But what do we mean by "nearby"? In the figure above, we have made an implicit decision of measuring "nearby" with Euclidean distance. This was a good idea, because the Euclidean distance between two states is highly correlated with the probability that the agent will transition between the two.
    </p>

    <p>
      However, it's easy to imagine cases where this implicit assumption breaks down. By adding a single long barrier, we can construct a case where the Euclidean distance metric leads to bad generalization. The problem is that we have merged the wrong paths.
    </p>

    <figure id="q-learning-intro" class="l-page" style="width: 100%; margin-top: 0px;">
      <center>
        <img style="width: 30%; margin: 15px; " src="figures/large-cliffworld-approx.svg"/>
        <img style="width: 30%; margin: 15px; " src="figures/large-cliffworld-barrier.svg"/>
          
      <figcaption class="l-body">
        <div class="eq-grid" style="grid-gap: 15%; width: 70%;">
          <div style="grid-row: 1; grid-column: 1;">
            Imagine changing the Cliff World setup by adding a long barrier.
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            Now the Euclidean distance metric gives bad generalization.
          </div>
        </div>
      </figcaption>
      </center>
    </figure>

    <p>
      <b>Merging the wrong paths.</b> The diagram below shows the ill effects of merging the wrong paths a bit more explicitly. Notice that, regardless of whether we use MC or TD learning, the function approximator gives a bad value estimate for the three states immediately above the barrier.
    </p>

    <figure class="l-page" style="width: 100%; margin-top: 15px;">
      <center>
        <img style="width: 70%; margin: 15px; " src="figures/compare-function-approx.svg"/>
        <figcaption class="l-body" style="width: 60%;">
          Effects of state approximation when the wrong paths are merged. Bad generalization can cause more severe errors under TD learning.
        </figcaption>
      </center>
    </figure>

    <p>
      Even though the averager is to blame for poor generalization, TD learning does much worse than Monte Carlo at handling them. This is the price we must pay for using TD learning: it is far more sensitive to these sorts of problems.
    </p>
  


    <h2>Implications for deep reinforcement learning</h2>

    <p>
      <b>Neural networks.</b> One of the most popular types of function approximator, especially in recent years, has been deep neural networks. These models are exciting for many reasons, but one particularly nice property is that they don't make implicit assumptions about which states are "nearby."
    </p>

    <p>
      <b>Learning the meaning of "nearby."</b> Early in training, neural networks, like averagers, tend to merge the wrong paths of experience. In the Cliff Walking example, an untrained neural network might make the same bad value updates as the Euclidean averager.
    </p>

    <p>
      But as training progresses, neural networks can actually learn to overcome these errors. They learn which states are "nearby" from experience. In the Cliff World example, we might expect a fully-trained neural network to have learned that value updates to states <i>above</i> the barrier should never affect the values of states <i>below</i> the barrier. This isn't something that most other function approximators can do. It's part of the reason why deep RL is so interesting!
    </p>

    <figure class="l-page" style="width: 100%; margin-top: 15px;">
      <center>
        <img style="width: 80%; margin: 15px; " src="figures/latent-distance.png"/>
      <figcaption class="l-body">
        <div class="eq-grid" style="grid-gap: 3%; width: 80%;">
          <div style="grid-row: 1; grid-column: 1;">
            A robotic hand, controlled by a neural network, that has learned its own concept of which states are "nearby."
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            Dark blue circles are states that the agent thinks are "nearby" while the light blue circles are states that are far away.
          </div>
          <div style="grid-row: 1; grid-column: 3;">
            Notice that the shape and location of obstacles affects the agent's notion of distance.
          </div>
        </div>
      </figcaption>
      </center>
    </figure>

    <p>
      <b>The best of both worlds?</b> So far, we have seen that TD learning has a clear advantage over Monte Carlo in tabular settings. But for different reasons, Monte Carlo can sometimes beat TD learning (or at least compete with it) when we use function approximation. Even though neural networks can learn to overcome bad generalization, it would be better if we were able to reduce bad generalization in the first place.
    </p>

    <p>
      That's the reasoning behind TD($\lambda$) learning. It's a technique that simply interpolates (using the coefficient $\lambda$) between Monte Carlo and TD updates. <d-footnote>In the limit $\lambda=0$, we recover the TD update rule. Meanwhile, when $\lambda=1$, we recover Monte Carlo.</d-footnote> Often, TD($\lambda$) works better than either Monte Carlo or TD learning alone.

    <p>
      Researchers usually keep the $\lambda$ coefficient constant when they train a deep RL model. However, if Monte Carlo learning is best early in training (before the agent has learned a good state representation) and TD learning is best later on (when it's easier to benefit from merging paths), maybe we should gradually decrease $\lambda$ over the course of training...
    </p>

    <p>
      Perhaps there are other ways for deep RL agents to combine the the stability of Monte Carlo with the efficiency of TD learning. Deep RL is still a young area of research and there are many open questions. We hope this post clarifies the relationship between Monte Carlo and TD and encourages readers to push the comparison even further.
    </p>




    <!-- <h2>Playground</h2>
    <br>

    <p>
      In order to build intuition about how RL works, in particular about the relationship between various types of value learning, we encourage you to try training your own Gridworld agent in the playground below.
    </p>

    <p>
      TODO: Explain anything in the demo that we haven't touched on in the main body of the article.
    </p>

    <div id="playground" style="width:600px; height:450px; position:relative;"> </div>
    <script src="vis/Playground.js" charset="utf-8"></script> -->







    <div id="appendix-a"></div>
    <h2>Appendix A: When does TD have lower variance than Monte Carlo?</h2>

    <p>
      In the process of writing this post, we found it useful to re-write the pseudocode for Monte Carlo and TD learning in order to highlight formal connections between the two. Our versions of the algorithms are <i>slightly</i> different from those presented in Sutton.
    </p>

    <p>
      An exciting side effect was that, using our version of the two algorithms, we were able to show that TD learning will, on average, always converge as fast or faster than Monte Carlo learning for <i>any</i> RL agent that uses a tabular value function. We could not find any proofs of this sort online, so we wrote our own.
    </p>

    <p>
      The full text is <a href="./files/Variance_TD_vs_MC.pdf">here</a>.
    </p>

</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
  </p>


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
